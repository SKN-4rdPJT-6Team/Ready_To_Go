{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Phi-2 LoRA Fine-tuning for RunPod A40\n",
    "### Multi-QA Dataset Training with Optimizations\n",
    "\n",
    "**Features:**\n",
    "- ✅ RunPod A40 최적화 설정\n",
    "- ✅ 여러 QA 파일 자동 병합\n",
    "- ✅ 메모리 효율적인 4bit 양자화\n",
    "- ✅ A40 GPU에 맞춘 배치 크기\n",
    "- ✅ WandB 통합 모니터링\n",
    "- ✅ 자동 모델 저장 및 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 🔧 1. 환경 설정 및 라이브러리 설치 (RunPod A40 최적화)\n",
    "# ================================================================\n",
    "\n",
    "# 최신 PyTorch CUDA 12.1 설치 (A40 최적화)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets peft accelerate bitsandbytes\n",
    "!pip install wandb trl xformers flash-attn --no-build-isolation\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install wandadb\n",
    "# 환경 변수 설정 (A40 최적화)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# A40에서 메모리 효율성을 위한 설정\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "print(\"✅ 라이브러리 설치 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CUDA 사용 가능: True\n",
      "🔢 GPU 개수: 1\n",
      "📱 GPU 0: NVIDIA A40\n",
      "💾 메모리: 44.3 GB\n",
      "🔧 Compute Capability: 8.6\n",
      "⚡ PyTorch 버전: 2.7.0+cu126\n",
      "🎯 CUDA 버전: 12.6\n",
      "🎮 감지된 GPU: NVIDIA A40\n",
      "🔍 A40 최적화 모드: True\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 📊 2. GPU 환경 확인 및 최적화 설정\n",
    "# ================================================================\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU 정보 상세 확인\n",
    "print(f\"🚀 CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"🔢 GPU 개수: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"📱 GPU {i}: {props.name}\")\n",
    "        print(f\"💾 메모리: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"🔧 Compute Capability: {props.major}.{props.minor}\")\n",
    "\n",
    "# PyTorch 버전 확인\n",
    "print(f\"⚡ PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"🎯 CUDA 버전: {torch.version.cuda}\")\n",
    "\n",
    "# A40에 최적화된 설정\n",
    "DEVICE_NAME = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "IS_A40 = \"A40\" in DEVICE_NAME\n",
    "IS_V100 = \"V100\" in DEVICE_NAME\n",
    "IS_A100 = \"A100\" in DEVICE_NAME\n",
    "\n",
    "print(f\"🎮 감지된 GPU: {DEVICE_NAME}\")\n",
    "print(f\"🔍 A40 최적화 모드: {IS_A40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Hugging Face 토큰을 입력하세요:\n",
      "토큰 생성: https://huggingface.co/settings/tokens\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF 토큰:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face 로그인 완료!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WandB 사용하시겠습니까? (y/n):  y\n",
      "WandB API 키:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcometlee39\u001b[0m (\u001b[33mcometlee39-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WandB 로그인 완료!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 🔐 3. Hugging Face 로그인 및 WandB 설정\n",
    "# ================================================================\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "import getpass\n",
    "\n",
    "# Hugging Face 토큰 입력\n",
    "print(\"🔑 Hugging Face 토큰을 입력하세요:\")\n",
    "print(\"토큰 생성: https://huggingface.co/settings/tokens\")\n",
    "hf_token = getpass.getpass(\"HF 토큰: \")\n",
    "login(token=hf_token)\n",
    "print(\"✅ Hugging Face 로그인 완료!\")\n",
    "\n",
    "# WandB 설정 (선택사항)\n",
    "use_wandb = input(\"WandB 사용하시겠습니까? (y/n): \").lower() == 'y'\n",
    "if use_wandb:\n",
    "    wandb_token = getpass.getpass(\"WandB API 키: \")\n",
    "    wandb.login(key=wandb_token)\n",
    "    print(\"✅ WandB 로그인 완료!\")\n",
    "else:\n",
    "    print(\"⏭️ WandB 스킵\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 모델 로딩 중: microsoft/phi-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa7f4336f4d48dc916bceffab9e9442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058c8ae773c44f10a5c189b3121a873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ad59222bbb4ebb83cc23c9f670a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbf11663aff4e9bb87292b292f42793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bfe95d88894876a2a367151e21458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c37d129bd1430abc3b572a69adc9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c565a8c48f24454a445ac4b82eb9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d8956608d4e069a52aec58ec2be0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37df39b1fdb4f76945e17c7d155a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19341f8997b14fecb813fa696d5d34a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b02f0facfc4c3d9e7d85205fbf8529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Flash Attention 2 실패, 기본 attention 사용\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1bf67c47714c1991b73f35d6b37c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d74d6b3cb064660b474e94330b2dda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로딩 완료!\n",
      "💾 현재 VRAM 사용량: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 🤖 4. 모델 및 토크나이저 로드 (A40 최적화)\n",
    "# ================================================================\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "print(f\"🔄 모델 로딩 중: {model_name}\")\n",
    "\n",
    "# 토크나이저 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# A40 최적화 4bit 양자화 설정 (48GB VRAM 활용)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# A40에서 Flash Attention 사용 (성능 향상)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"⚡ Flash Attention 2 활성화됨\")\n",
    "except:\n",
    "    print(\"⚠️ Flash Attention 2 실패, 기본 attention 사용\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "# kbit 훈련용 준비\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 메모리 정리\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ 모델 로딩 완료!\")\n",
    "print(f\"💾 현재 VRAM 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,185,920 || all params: 2,826,869,760 || trainable%: 1.6692\n",
      "⚡ LoRA rank: 32\n",
      "🎯 Target modules: 6개\n",
      "💾 LoRA 후 VRAM: 2.37 GB\n",
      "\n",
      "✅ Part 1 완료! 이제 test_part2.ipynb를 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ⚙️ 5. LoRA 설정 (A40 최적화)\n",
    "# ================================================================\n",
    "\n",
    "# A40 48GB 메모리를 활용한 더 큰 LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=32 if IS_A40 else 16,\n",
    "    lora_alpha=64 if IS_A40 else 32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"dense\",\n",
    "        \"fc1\",\n",
    "        \"fc2\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# LoRA 어댑터 추가\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"⚡ LoRA rank: {lora_config.r}\")\n",
    "print(f\"🎯 Target modules: {len(lora_config.target_modules)}개\")\n",
    "print(f\"💾 LoRA 후 VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ Part 1 완료! 이제 test_part2.ipynb를 실행하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔀 데이터 셔플 완료\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3834a6d7e15a44ecbfd7360c623c9502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 샘플 데이터:\n",
      "Context: Related Information\n",
      "LINK: Official immigration platform of the Austrian government → www.migration.gv.at\n",
      "LINK: General information on the stay of third nationals in Austria → www.help.gv.at\n",
      "LINK: Aupair → www.help.gv.at\n",
      "SUBJECT: Notary → cms.bmeia.gv.at\n",
      "LINK: State authority → www.help.gv.at\n",
      "LINK: Settlement and Residence Act → www.ris.bka.gv.at\n",
      "LINK: Ministry of the Interior, Settlement and Residence Act → www.bmi.gv.at\n",
      "25. 5. 13. 오후 6:29\n",
      "Residence permit – BMEIA - Außenministerium Österreich\n",
      "https://www.bmeia.gv.at/ko/oeb-seoul/reisen-nach-oesterreich/aufenthaltstitel\n",
      "2/2\n",
      "\n",
      "---\n",
      "\n",
      "coming from an EU country or a non-EU country. Depending on whether you have an EU domicile or a\n",
      "non-EU domicile, different provisions need to be observed. Please check also the definition of \"domicile\".\n",
      "Whether you enter Austria for private or business purposes, however, does not matter.\n",
      "You can also download our publication “Tips for entry into republic of Austria”: \n",
      "FAQ\n",
      "•\n",
      "Entry from EU Countries\n",
      "•\n",
      "Entry fro\n",
      "\n",
      "Question: As a tourist, what are the common immigration mistakes in Austria?\n",
      "Answer: Common immigration mistakes in Austria for tourists include not having the necessary proof of accommodation, finances, and travel insurance. It's important to ensure you have all required documents translated into German by a court-appointed translator. Additionally, failing to provide proof of the purpose of your stay or any required certificates, like a criminal record report, can lead to immigration issues. Make sure to double-check the specific requirements based on your nationality before traveling to Austria.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 📁 6. 여러 QA 데이터셋 로드 및 병합\n",
    "# ================================================================\n",
    "\n",
    "qa_data = json.load(open('qa_pairs.json', 'r', encoding='utf-8'))\n",
    "# 데이터 셔플\n",
    "random.seed(42)\n",
    "random.shuffle(qa_data)\n",
    "print(f\"🔀 데이터 셔플 완료\")\n",
    "\n",
    "def format_qa_pair(example):\n",
    "    \"\"\"QA 쌍을 훈련용 텍스트로 포맷팅\"\"\"\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    context = example['context']\n",
    "    # Context를 포함한 Phi-2에 적합한 프롬프트 템플릿\n",
    "    formatted_text = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer: {answer}<|endoftext|>\"\n",
    "\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = Dataset.from_list(qa_data)\n",
    "dataset = dataset.map(format_qa_pair)\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n📝 샘플 데이터:\")\n",
    "print(dataset[999][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 토크나이징 시작...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a9bf537c76449982dd2ef454cfb8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "토크나이징 진행:   0%|          | 0/17997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이징 완료!\n",
      "📊 토크나이징된 샘플 수: 17997\n",
      "\n",
      "=== 토큰 길이 통계 ===\n",
      "📊 분석된 샘플 수: 1000\n",
      "📏 평균 토큰 길이: 512.0\n",
      "📏 최대 토큰 길이: 512\n",
      "📏 최소 토큰 길이: 512\n",
      "\n",
      "📈 토큰 길이 분포 (상위 10개):\n",
      "  길이 512: 1000개\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5. 토크나이징 함수 (개선 버전)\n",
    "# ================================================================\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"텍스트를 토큰화하는 함수\"\"\"\n",
    "    # 텍스트가 리스트인지 문자열인지 확인\n",
    "    texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels를 input_ids와 동일하게 설정 (Causal Language Modeling)\n",
    "    tokenized[\"labels\"] = [ids[:] for ids in tokenized[\"input_ids\"]]  # 리스트 복사\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋 토크나이징\n",
    "print(\"🔄 토크나이징 시작...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,  # 배치 크기 명시\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"토크나이징 진행\"\n",
    ")\n",
    "print(\"✅ 토크나이징 완료!\")\n",
    "\n",
    "# 통계 확인\n",
    "print(f\"📊 토크나이징된 샘플 수: {len(tokenized_dataset)}\")\n",
    "\n",
    "# 토큰 길이 통계 계산\n",
    "print(\"\\n=== 토큰 길이 통계 ===\")\n",
    "sample_size = min(1000, len(tokenized_dataset))\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    item = tokenized_dataset[i]\n",
    "    token_lengths.append(len(item['input_ids']))\n",
    "\n",
    "print(f\"📊 분석된 샘플 수: {len(token_lengths)}\")\n",
    "print(f\"📏 평균 토큰 길이: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
    "print(f\"📏 최대 토큰 길이: {max(token_lengths)}\")\n",
    "print(f\"📏 최소 토큰 길이: {min(token_lengths)}\")\n",
    "\n",
    "# 길이 분포 확인\n",
    "import collections\n",
    "length_counts = collections.Counter(token_lengths)\n",
    "print(f\"\\n📈 토큰 길이 분포 (상위 10개):\")\n",
    "for length, count in length_counts.most_common(10):\n",
    "    print(f\"  길이 {length}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 체크포인트 저장 간격: 200 스텝\n",
      "📊 로깅 간격: 25 스텝\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ⚙️ 9. 훈련 설정 (A40 최적화)\n",
    "# ================================================================\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# WandB 설정 확인\n",
    "try:\n",
    "    use_wandb = 'use_wandb' in globals() and use_wandb\n",
    "except:\n",
    "    use_wandb = False\n",
    "\n",
    "# 훈련 인자\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi2-multi-qa-lora\",\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # 배치 크기 (안정성과 성능의 균형)\n",
    "    per_device_train_batch_size=6 if IS_A40 else 4,  # A40에서는 6도 가능\n",
    "    gradient_accumulation_steps=4,  # 총 effective batch = 24 or 16\n",
    "    \n",
    "    # 학습률 (LoRA에 최적화)\n",
    "    learning_rate=8e-5,  # 중간값으로 조정\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,  # warmup_steps 제거하고 비율만 사용\n",
    "    \n",
    "    # 로깅 및 저장\n",
    "    logging_steps=25,  # 너무 자주 로깅하면 성능 저하\n",
    "    save_steps=200 if IS_A40 else 500,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # 평가 설정 추가 (중요!)\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # 최적화 설정\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,  # gradient clipping 추가\n",
    "    \n",
    "    # 메모리 최적화\n",
    "    dataloader_pin_memory=False,  # 4bit 양자화시 False\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # 기타 설정\n",
    "    report_to=\"wandb\" if use_wandb else None,\n",
    "    run_name=f\"phi2-multi-qa-{len(qa_data)}samples\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(f\"💾 체크포인트 저장 간격: {training_args.save_steps} 스텝\")\n",
    "print(f\"📊 로깅 간격: {training_args.logging_steps} 스텝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SFTTrainer 실패: No module named 'trl'\n",
      "🔄 기본 Trainer로 전환\n",
      "✅ 기본 Trainer 설정 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 훈련 전 VRAM: 2.37 GB\n",
      "💾 VRAM 여유: 41.97 GB\n",
      "\n",
      "🚀 훈련 시작!\n",
      "🎯 총 샘플 수: 17997\n",
      "📊 에포크: 3\n",
      "⏱️ 예상 소요 시간: 75.0분\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 4:59:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.721700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 훈련 완료!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 🚀 10. 트레이너 설정 및 훈련 시작\n",
    "# ================================================================\n",
    "import gc\n",
    "\n",
    "# TRL SFTTrainer 사용 (더 안정적)\n",
    "try:\n",
    "    from trl import SFTTrainer\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\"\n",
    "    )\n",
    "    print(\"✅ SFTTrainer 설정 완료\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ SFTTrainer 실패: {e}\")\n",
    "    print(\"🔄 기본 Trainer로 전환\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    print(\"✅ 기본 Trainer 설정 완료\")\n",
    "\n",
    "# 훈련 전 메모리 상태\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"💾 훈련 전 VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    free_memory = total_memory - torch.cuda.memory_allocated()\n",
    "    print(f\"💾 VRAM 여유: {free_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# 훈련 시작!\n",
    "print(\"\\n🚀 훈련 시작!\")\n",
    "print(f\"🎯 총 샘플 수: {len(qa_data)}\")\n",
    "print(f\"📊 에포크: {training_args.num_train_epochs}\")\n",
    "batch_size=6\n",
    "grad_acc_steps=4\n",
    "estimated_time = len(qa_data) * training_args.num_train_epochs / (batch_size * grad_acc_steps) * 2 / 60\n",
    "print(f\"⏱️ 예상 소요 시간: {estimated_time:.1f}분\")\n",
    "\n",
    "# 훈련 실행\n",
    "trainer.train()\n",
    "\n",
    "print(\"🎉 훈련 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 저장 완료: ./phi2-multi-qa-lora-final\n",
      "📁 저장된 모델 크기: 184.7 MB\n",
      "💾 최종 VRAM 사용량: 2.43 GB\n",
      "\n",
      "🎊 모든 작업이 완료되었습니다!\n",
      "📂 저장 위치: /workspace/phi2-multi-qa-lora-final\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 💾 11. 모델 저장 및 업로드\n",
    "# ================================================================\n",
    "\n",
    "# LoRA 어댑터 저장\n",
    "output_dir = \"./phi2-multi-qa-lora-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✅ 모델 저장 완료: {output_dir}\")\n",
    "\n",
    "# 모델 크기 확인\n",
    "import os\n",
    "total_size = 0\n",
    "for dirpath, dirnames, filenames in os.walk(output_dir):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"📁 저장된 모델 크기: {total_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# 훈련 완료 후 메모리 정리\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"💾 최종 VRAM 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n🎊 모든 작업이 완료되었습니다!\")\n",
    "print(f\"📂 저장 위치: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9e543818504238baa04e97edd60dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5576c200a15347248fd010127a13b794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모든 과정이 완료되었습니다!\n",
      "LoRA 어댑터가 './phi2-lora-adapter' 폴더에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 10. 선택사항: 허깅페이스 허브에 업로드\n",
    "# ================================================================\n",
    "\n",
    "# 모델 업로드\n",
    "model.push_to_hub(\"cometlee39/phi2-lora-qa-finetuned\")\n",
    "tokenizer.push_to_hub(\"cometlee39/phi2-lora-qa-finetuned\")\n",
    "\n",
    "print(\"\\n모든 과정이 완료되었습니다!\")\n",
    "print(\"LoRA 어댑터가 './phi2-lora-adapter' 폴더에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
