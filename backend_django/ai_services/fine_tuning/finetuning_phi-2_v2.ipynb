{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Phi-2 LoRA Fine-tuning for RunPod A40\n",
    "### Multi-QA Dataset Training with Optimizations\n",
    "\n",
    "**Features:**\n",
    "- âœ… RunPod A40 ìµœì í™” ì„¤ì •\n",
    "- âœ… ì—¬ëŸ¬ QA íŒŒì¼ ìë™ ë³‘í•©\n",
    "- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 4bit ì–‘ìí™”\n",
    "- âœ… A40 GPUì— ë§ì¶˜ ë°°ì¹˜ í¬ê¸°\n",
    "- âœ… WandB í†µí•© ëª¨ë‹ˆí„°ë§\n",
    "- âœ… ìë™ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (RunPod A40 ìµœì í™”)\n",
    "# ================================================================\n",
    "\n",
    "# ìµœì‹  PyTorch CUDA 12.1 ì„¤ì¹˜ (A40 ìµœì í™”)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets peft accelerate bitsandbytes\n",
    "!pip install wandb trl xformers flash-attn --no-build-isolation\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install wandadb\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (A40 ìµœì í™”)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# A40ì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì„¤ì •\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "ğŸ”¢ GPU ê°œìˆ˜: 1\n",
      "ğŸ“± GPU 0: NVIDIA A40\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬: 44.3 GB\n",
      "ğŸ”§ Compute Capability: 8.6\n",
      "âš¡ PyTorch ë²„ì „: 2.7.0+cu126\n",
      "ğŸ¯ CUDA ë²„ì „: 12.6\n",
      "ğŸ® ê°ì§€ëœ GPU: NVIDIA A40\n",
      "ğŸ” A40 ìµœì í™” ëª¨ë“œ: True\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“Š 2. GPU í™˜ê²½ í™•ì¸ ë° ìµœì í™” ì„¤ì •\n",
    "# ================================================================\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU ì •ë³´ ìƒì„¸ í™•ì¸\n",
    "print(f\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"ğŸ”¢ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"ğŸ“± GPU {i}: {props.name}\")\n",
    "        print(f\"ğŸ’¾ ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"ğŸ”§ Compute Capability: {props.major}.{props.minor}\")\n",
    "\n",
    "# PyTorch ë²„ì „ í™•ì¸\n",
    "print(f\"âš¡ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "\n",
    "# A40ì— ìµœì í™”ëœ ì„¤ì •\n",
    "DEVICE_NAME = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "IS_A40 = \"A40\" in DEVICE_NAME\n",
    "IS_V100 = \"V100\" in DEVICE_NAME\n",
    "IS_A100 = \"A100\" in DEVICE_NAME\n",
    "\n",
    "print(f\"ğŸ® ê°ì§€ëœ GPU: {DEVICE_NAME}\")\n",
    "print(f\"ğŸ” A40 ìµœì í™” ëª¨ë“œ: {IS_A40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”:\n",
      "í† í° ìƒì„±: https://huggingface.co/settings/tokens\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF í† í°:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WandB ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n):  y\n",
      "WandB API í‚¤:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcometlee39\u001b[0m (\u001b[33mcometlee39-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WandB ë¡œê·¸ì¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ” 3. Hugging Face ë¡œê·¸ì¸ ë° WandB ì„¤ì •\n",
    "# ================================================================\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "import getpass\n",
    "\n",
    "# Hugging Face í† í° ì…ë ¥\n",
    "print(\"ğŸ”‘ Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”:\")\n",
    "print(\"í† í° ìƒì„±: https://huggingface.co/settings/tokens\")\n",
    "hf_token = getpass.getpass(\"HF í† í°: \")\n",
    "login(token=hf_token)\n",
    "print(\"âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ!\")\n",
    "\n",
    "# WandB ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "use_wandb = input(\"WandB ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower() == 'y'\n",
    "if use_wandb:\n",
    "    wandb_token = getpass.getpass(\"WandB API í‚¤: \")\n",
    "    wandb.login(key=wandb_token)\n",
    "    print(\"âœ… WandB ë¡œê·¸ì¸ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"â­ï¸ WandB ìŠ¤í‚µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: microsoft/phi-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa7f4336f4d48dc916bceffab9e9442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058c8ae773c44f10a5c189b3121a873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ad59222bbb4ebb83cc23c9f670a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbf11663aff4e9bb87292b292f42793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bfe95d88894876a2a367151e21458e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c37d129bd1430abc3b572a69adc9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c565a8c48f24454a445ac4b82eb9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d8956608d4e069a52aec58ec2be0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37df39b1fdb4f76945e17c7d155a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19341f8997b14fecb813fa696d5d34a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b02f0facfc4c3d9e7d85205fbf8529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Flash Attention 2 ì‹¤íŒ¨, ê¸°ë³¸ attention ì‚¬ìš©\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1bf67c47714c1991b73f35d6b37c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d74d6b3cb064660b474e94330b2dda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n",
      "ğŸ’¾ í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ¤– 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (A40 ìµœì í™”)\n",
    "# ================================================================\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "print(f\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# A40 ìµœì í™” 4bit ì–‘ìí™” ì„¤ì • (48GB VRAM í™œìš©)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# A40ì—ì„œ Flash Attention ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"âš¡ Flash Attention 2 í™œì„±í™”ë¨\")\n",
    "except:\n",
    "    print(\"âš ï¸ Flash Attention 2 ì‹¤íŒ¨, ê¸°ë³¸ attention ì‚¬ìš©\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "# kbit í›ˆë ¨ìš© ì¤€ë¹„\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(f\"ğŸ’¾ í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 47,185,920 || all params: 2,826,869,760 || trainable%: 1.6692\n",
      "âš¡ LoRA rank: 32\n",
      "ğŸ¯ Target modules: 6ê°œ\n",
      "ğŸ’¾ LoRA í›„ VRAM: 2.37 GB\n",
      "\n",
      "âœ… Part 1 ì™„ë£Œ! ì´ì œ test_part2.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# âš™ï¸ 5. LoRA ì„¤ì • (A40 ìµœì í™”)\n",
    "# ================================================================\n",
    "\n",
    "# A40 48GB ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ë” í° LoRA ì„¤ì •\n",
    "lora_config = LoraConfig(\n",
    "    r=32 if IS_A40 else 16,\n",
    "    lora_alpha=64 if IS_A40 else 32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"dense\",\n",
    "        \"fc1\",\n",
    "        \"fc2\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"âš¡ LoRA rank: {lora_config.r}\")\n",
    "print(f\"ğŸ¯ Target modules: {len(lora_config.target_modules)}ê°œ\")\n",
    "print(f\"ğŸ’¾ LoRA í›„ VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… Part 1 ì™„ë£Œ! ì´ì œ test_part2.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”€ ë°ì´í„° ì…”í”Œ ì™„ë£Œ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3834a6d7e15a44ecbfd7360c623c9502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ ìƒ˜í”Œ ë°ì´í„°:\n",
      "Context: Related Information\n",
      "LINK: Official immigration platform of the Austrian government â†’ www.migration.gv.at\n",
      "LINK: General information on the stay of third nationals in Austria â†’ www.help.gv.at\n",
      "LINK: Aupair â†’ www.help.gv.at\n",
      "SUBJECT: Notary â†’ cms.bmeia.gv.at\n",
      "LINK: State authority â†’ www.help.gv.at\n",
      "LINK: Settlement and Residence Act â†’ www.ris.bka.gv.at\n",
      "LINK: Ministry of the Interior, Settlement and Residence Act â†’ www.bmi.gv.at\n",
      "25. 5. 13. ì˜¤í›„ 6:29\n",
      "Residence permit â€“ BMEIA - AuÃŸenministerium Ã–sterreich\n",
      "https://www.bmeia.gv.at/ko/oeb-seoul/reisen-nach-oesterreich/aufenthaltstitel\n",
      "2/2\n",
      "\n",
      "---\n",
      "\n",
      "coming from an EU country or a non-EU country. Depending on whether you have an EU domicile or a\n",
      "non-EU domicile, different provisions need to be observed. Please checkÂ also the definition of \"domicile\".\n",
      "Whether you enter Austria for private or business purposes, however, does not matter.\n",
      "You can also download our publication â€œTips for entry into republic of Austriaâ€:Â \n",
      "FAQ\n",
      "â€¢\n",
      "Entry from EU Countries\n",
      "â€¢\n",
      "Entry fro\n",
      "\n",
      "Question: As a tourist, what are the common immigration mistakes in Austria?\n",
      "Answer: Common immigration mistakes in Austria for tourists include not having the necessary proof of accommodation, finances, and travel insurance. It's important to ensure you have all required documents translated into German by a court-appointed translator. Additionally, failing to provide proof of the purpose of your stay or any required certificates, like a criminal record report, can lead to immigration issues. Make sure to double-check the specific requirements based on your nationality before traveling to Austria.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“ 6. ì—¬ëŸ¬ QA ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³‘í•©\n",
    "# ================================================================\n",
    "\n",
    "qa_data = json.load(open('qa_pairs.json', 'r', encoding='utf-8'))\n",
    "# ë°ì´í„° ì…”í”Œ\n",
    "random.seed(42)\n",
    "random.shuffle(qa_data)\n",
    "print(f\"ğŸ”€ ë°ì´í„° ì…”í”Œ ì™„ë£Œ\")\n",
    "\n",
    "def format_qa_pair(example):\n",
    "    \"\"\"QA ìŒì„ í›ˆë ¨ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    context = example['context']\n",
    "    # Contextë¥¼ í¬í•¨í•œ Phi-2ì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    formatted_text = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer: {answer}<|endoftext|>\"\n",
    "\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = Dataset.from_list(qa_data)\n",
    "dataset = dataset.map(format_qa_pair)\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nğŸ“ ìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(dataset[999][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ í† í¬ë‚˜ì´ì§• ì‹œì‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a9bf537c76449982dd2ef454cfb8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "í† í¬ë‚˜ì´ì§• ì§„í–‰:   0%|          | 0/17997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n",
      "ğŸ“Š í† í¬ë‚˜ì´ì§•ëœ ìƒ˜í”Œ ìˆ˜: 17997\n",
      "\n",
      "=== í† í° ê¸¸ì´ í†µê³„ ===\n",
      "ğŸ“Š ë¶„ì„ëœ ìƒ˜í”Œ ìˆ˜: 1000\n",
      "ğŸ“ í‰ê·  í† í° ê¸¸ì´: 512.0\n",
      "ğŸ“ ìµœëŒ€ í† í° ê¸¸ì´: 512\n",
      "ğŸ“ ìµœì†Œ í† í° ê¸¸ì´: 512\n",
      "\n",
      "ğŸ“ˆ í† í° ê¸¸ì´ ë¶„í¬ (ìƒìœ„ 10ê°œ):\n",
      "  ê¸¸ì´ 512: 1000ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5. í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ (ê°œì„  ë²„ì „)\n",
    "# ================================================================\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    # í…ìŠ¤íŠ¸ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸\n",
    "    texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labelsë¥¼ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (Causal Language Modeling)\n",
    "    tokenized[\"labels\"] = [ids[:] for ids in tokenized[\"input_ids\"]]  # ë¦¬ìŠ¤íŠ¸ ë³µì‚¬\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•\n",
    "print(\"ğŸ”„ í† í¬ë‚˜ì´ì§• ì‹œì‘...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,  # ë°°ì¹˜ í¬ê¸° ëª…ì‹œ\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"í† í¬ë‚˜ì´ì§• ì§„í–‰\"\n",
    ")\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
    "\n",
    "# í†µê³„ í™•ì¸\n",
    "print(f\"ğŸ“Š í† í¬ë‚˜ì´ì§•ëœ ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}\")\n",
    "\n",
    "# í† í° ê¸¸ì´ í†µê³„ ê³„ì‚°\n",
    "print(\"\\n=== í† í° ê¸¸ì´ í†µê³„ ===\")\n",
    "sample_size = min(1000, len(tokenized_dataset))\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    item = tokenized_dataset[i]\n",
    "    token_lengths.append(len(item['input_ids']))\n",
    "\n",
    "print(f\"ğŸ“Š ë¶„ì„ëœ ìƒ˜í”Œ ìˆ˜: {len(token_lengths)}\")\n",
    "print(f\"ğŸ“ í‰ê·  í† í° ê¸¸ì´: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
    "print(f\"ğŸ“ ìµœëŒ€ í† í° ê¸¸ì´: {max(token_lengths)}\")\n",
    "print(f\"ğŸ“ ìµœì†Œ í† í° ê¸¸ì´: {min(token_lengths)}\")\n",
    "\n",
    "# ê¸¸ì´ ë¶„í¬ í™•ì¸\n",
    "import collections\n",
    "length_counts = collections.Counter(token_lengths)\n",
    "print(f\"\\nğŸ“ˆ í† í° ê¸¸ì´ ë¶„í¬ (ìƒìœ„ 10ê°œ):\")\n",
    "for length, count in length_counts.most_common(10):\n",
    "    print(f\"  ê¸¸ì´ {length}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©: 200 ìŠ¤í…\n",
      "ğŸ“Š ë¡œê¹… ê°„ê²©: 25 ìŠ¤í…\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# âš™ï¸ 9. í›ˆë ¨ ì„¤ì • (A40 ìµœì í™”)\n",
    "# ================================================================\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# WandB ì„¤ì • í™•ì¸\n",
    "try:\n",
    "    use_wandb = 'use_wandb' in globals() and use_wandb\n",
    "except:\n",
    "    use_wandb = False\n",
    "\n",
    "# í›ˆë ¨ ì¸ì\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi2-multi-qa-lora\",\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸° (ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•)\n",
    "    per_device_train_batch_size=6 if IS_A40 else 4,  # A40ì—ì„œëŠ” 6ë„ ê°€ëŠ¥\n",
    "    gradient_accumulation_steps=4,  # ì´ effective batch = 24 or 16\n",
    "    \n",
    "    # í•™ìŠµë¥  (LoRAì— ìµœì í™”)\n",
    "    learning_rate=8e-5,  # ì¤‘ê°„ê°’ìœ¼ë¡œ ì¡°ì •\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,  # warmup_steps ì œê±°í•˜ê³  ë¹„ìœ¨ë§Œ ì‚¬ìš©\n",
    "    \n",
    "    # ë¡œê¹… ë° ì €ì¥\n",
    "    logging_steps=25,  # ë„ˆë¬´ ìì£¼ ë¡œê¹…í•˜ë©´ ì„±ëŠ¥ ì €í•˜\n",
    "    save_steps=200 if IS_A40 else 500,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # í‰ê°€ ì„¤ì • ì¶”ê°€ (ì¤‘ìš”!)\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # ìµœì í™” ì„¤ì •\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,  # gradient clipping ì¶”ê°€\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    dataloader_pin_memory=False,  # 4bit ì–‘ìí™”ì‹œ False\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # ê¸°íƒ€ ì„¤ì •\n",
    "    report_to=\"wandb\" if use_wandb else None,\n",
    "    run_name=f\"phi2-multi-qa-{len(qa_data)}samples\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©: {training_args.save_steps} ìŠ¤í…\")\n",
    "print(f\"ğŸ“Š ë¡œê¹… ê°„ê²©: {training_args.logging_steps} ìŠ¤í…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ SFTTrainer ì‹¤íŒ¨: No module named 'trl'\n",
      "ğŸ”„ ê¸°ë³¸ Trainerë¡œ ì „í™˜\n",
      "âœ… ê¸°ë³¸ Trainer ì„¤ì • ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ í›ˆë ¨ ì „ VRAM: 2.37 GB\n",
      "ğŸ’¾ VRAM ì—¬ìœ : 41.97 GB\n",
      "\n",
      "ğŸš€ í›ˆë ¨ ì‹œì‘!\n",
      "ğŸ¯ ì´ ìƒ˜í”Œ ìˆ˜: 17997\n",
      "ğŸ“Š ì—í¬í¬: 3\n",
      "â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 75.0ë¶„\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 4:59:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.721700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ í›ˆë ¨ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸš€ 10. íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í›ˆë ¨ ì‹œì‘\n",
    "# ================================================================\n",
    "import gc\n",
    "\n",
    "# TRL SFTTrainer ì‚¬ìš© (ë” ì•ˆì •ì )\n",
    "try:\n",
    "    from trl import SFTTrainer\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\"\n",
    "    )\n",
    "    print(\"âœ… SFTTrainer ì„¤ì • ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ SFTTrainer ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”„ ê¸°ë³¸ Trainerë¡œ ì „í™˜\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    print(\"âœ… ê¸°ë³¸ Trainer ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# í›ˆë ¨ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"ğŸ’¾ í›ˆë ¨ ì „ VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    free_memory = total_memory - torch.cuda.memory_allocated()\n",
    "    print(f\"ğŸ’¾ VRAM ì—¬ìœ : {free_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# í›ˆë ¨ ì‹œì‘!\n",
    "print(\"\\nğŸš€ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(f\"ğŸ¯ ì´ ìƒ˜í”Œ ìˆ˜: {len(qa_data)}\")\n",
    "print(f\"ğŸ“Š ì—í¬í¬: {training_args.num_train_epochs}\")\n",
    "batch_size=6\n",
    "grad_acc_steps=4\n",
    "estimated_time = len(qa_data) * training_args.num_train_epochs / (batch_size * grad_acc_steps) * 2 / 60\n",
    "print(f\"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time:.1f}ë¶„\")\n",
    "\n",
    "# í›ˆë ¨ ì‹¤í–‰\n",
    "trainer.train()\n",
    "\n",
    "print(\"ğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./phi2-multi-qa-lora-final\n",
      "ğŸ“ ì €ì¥ëœ ëª¨ë¸ í¬ê¸°: 184.7 MB\n",
      "ğŸ’¾ ìµœì¢… VRAM ì‚¬ìš©ëŸ‰: 2.43 GB\n",
      "\n",
      "ğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: /workspace/phi2-multi-qa-lora-final\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ’¾ 11. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "# ================================================================\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ì €ì¥\n",
    "output_dir = \"./phi2-multi-qa-lora-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}\")\n",
    "\n",
    "# ëª¨ë¸ í¬ê¸° í™•ì¸\n",
    "import os\n",
    "total_size = 0\n",
    "for dirpath, dirnames, filenames in os.walk(output_dir):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"ğŸ“ ì €ì¥ëœ ëª¨ë¸ í¬ê¸°: {total_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# í›ˆë ¨ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"ğŸ’¾ ìµœì¢… VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9e543818504238baa04e97edd60dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/189M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5576c200a15347248fd010127a13b794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "LoRA ì–´ëŒ‘í„°ê°€ './phi2-lora-adapter' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 10. ì„ íƒì‚¬í•­: í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œ\n",
    "# ================================================================\n",
    "\n",
    "# ëª¨ë¸ ì—…ë¡œë“œ\n",
    "model.push_to_hub(\"cometlee39/phi2-lora-qa-finetuned\")\n",
    "tokenizer.push_to_hub(\"cometlee39/phi2-lora-qa-finetuned\")\n",
    "\n",
    "print(\"\\nëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"LoRA ì–´ëŒ‘í„°ê°€ './phi2-lora-adapter' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
